# OA trained with RL using Depth Prediction as Critic

## Introduction

The goal is to train a DNN that can navigate an autonomous drone based on monocular images in the setting of obstacle avoidance.

This could be done with imitation learning. In that setting an expert or demonstrator could collect a big dataset from which a policy could be trained in a supervised learning setting. The problem with the expert is, that it does not necessarily demonstrate the task in a robust fashion. The moment a the trained policy arrives in a state that differs from the demonstrated data, it will not be able to recover to a known state. This bias makes training such a policy hard. 

A second drawback of behavioral cloning (supervised imitation learning) is that the expert might not represent the optimal policy and the DNN will only get as good as the demonstrator. (Aggrevate is an exception)

RL on the other hand could train an optimal policy, though RL algorithms tend to require a lot more data.

Training policies for autonomous navigation can be done with imitation learning or RL. Though it is often tricky to define a smooth, comprehensive and stable reward.

Our learning algorithm, inspired by RL algorithms, bootstraps the training procedure by first focussing on training a good value function, in this case an State-Action advantage function represented by a depth predictor.
Obstacle avoidance can be defined as maximizing the overal depth in an image by steering away from approaching obstacles. 

The Q-function represents the expected return (with horizon 1) given a certain state (RGB-image) and action (control). In order to train the Q-function we use Q-learning. The data is collected by a heuristic collecting Ground Truth depth. 
A DNN is trained to predict the next depth frame given current RGB frame and applied action.



DNN has shown to predict depth reasonably well. 

Depth has already been used for obstacle avoidance in quite some work; for instance as a feature as input, or as an auxiliary task.

**Remarks:**
* Using depth or optical flow? ==> optical flow can be calculated offline on drone data in order to finetune


In this work, we skip the reward step and let a depth map represent the state-action value function by predicting the depth of the next frame. 


## Method

**Why Depth As Q-function**
The Q-function is the expected accumulated return given a certain state and action. 
When a drone is navigating in a certain direction, the expected depth to be seen at the next step can represent the estimated successful flight. The closer objects appear, the smaller the estimated depth map and to more likely the drone will crash.

The feature extracting part can be pretrained on real depth images.

The depth generating part will be trained in simulation with data generated by an expert or demonstrator. This could also be collected by a random policy ensuring the coverage of the action space.

The offline and off-policy training of the depth-predictor or Q-function allows for the acquisition of more data. The training can be done highly parallel speeding up the process.

Once the Q-predictor is trained, a policy can be extracted with any RL technique. 

In the Q-learning setting, the policy has a discrete (low dimensional) action space. In this setting the Q-predictor could predict a depth map for each discrete action value. The policy entails in taking the maximum over the different future predicted depth maps.

In case of continuous action space, it is best to train the policy in an actor-critic fashion. In that case the gradients of the depth predicting part can flow through the action node to the actor part of the network. The actor could share the feature extracting part of the depth predictor to fasten the training. 

## Implementation steps

* Create basic Q-net architecture: Imagenet Pretrained Mobilenet 0.5 with depth prediction layers and action concatenation (today)
* Train Q-net to predict next frame and explore variance over different actions _crucial_ (tonight + wednesday)
* Adjust architecture for 3 discrete control values: -1, 0, 1 with 3 representative depth maps (thursday)
* Train Q-net to predict next 3 potential outcomes. (thursday night)
* Test policy by taking max over 3 potential predicted depth maps (friday)
	* --> can this fly in training world? (canyon)
	* --> can this fly in test world? (forest / esat)
	* --> can this steer in real world (bebop)






